{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convolution</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage, misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a two-dimensional convolution object by using the constructor Conv2d, the parameter <code>in_channels</code> and <code>out_channels</code> will be used for this section, and the parameter kernel_size will be three.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a 2D convolutional layer\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[ 1.,  0., -1.],\n",
       "                        [ 2.,  0., -2.],\n",
       "                        [ 1.,  0., -1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give some random values to weights and bias\n",
    "conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\n",
    "conv.state_dict()['bias'][0]=0.0\n",
    "conv.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dummy tensor to represent an image. The shape of the image is (1,1,5,5) where:\n",
    "\n",
    "(number of inputs, number of channels, number of rows, number of columns ) \n",
    "\n",
    "Set the third column to 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy tensor to represent an image\n",
    "image=torch.zeros(1,1,5,5)\n",
    "image[0,0,:,2]=1\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=conv(image)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Determining  the Size of the Output</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formula O = I-K+1\n",
    "K=2\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=K)\n",
    "conv1.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv1.state_dict()['bias'][0]=0.0\n",
    "conv1.state_dict()\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=4\n",
    "image1=torch.ones(1,1,I,I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1: tensor([[[[4., 4., 4.],\n",
      "          [4., 4., 4.],\n",
      "          [4., 4., 4.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# perform convolution\n",
    "z1=conv1(image1)\n",
    "print(\"z1:\",z1)\n",
    "print(\"shape:\",z1.shape[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stride parameter</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a convolution object with a stride of 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[1., 1.],\n",
       "                        [1., 1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formula changes to O = (I-K)/S+1\n",
    "\n",
    "conv3 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=2)\n",
    "\n",
    "conv3.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv3.state_dict()['bias'][0]=0.0\n",
    "conv3.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z3: tensor([[[[4., 4.],\n",
      "          [4., 4.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "z3=conv3(image1)\n",
    "\n",
    "print(\"z3:\",z3)\n",
    "print(\"shape:\",z3.shape[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Zero Padding </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z4: tensor([[[[4.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "z4: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# perform convolutions with the kernel_size=2 and a stride=3\n",
    "# formula gives -> 1.66\n",
    "conv4 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\n",
    "conv4.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv4.state_dict()['bias'][0]=0.0\n",
    "conv4.state_dict()\n",
    "z4=conv4(image1)\n",
    "print(\"z4:\",z4)\n",
    "print(\"z4:\",z4.shape[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add rows and columns of zeros around the image. This is called padding. In the constructor <code>Conv2d</code>, you specify the number of rows or columns of zeros that you want to add with the parameter padding. \n",
    "\n",
    "For a square image, you merely pad an extra column of zeros to the first column and the last column. Repeat the process for the rows. As a result, for a square image, the width and height is the original size plus 2 x the number of padding elements specified. You can then determine the size of the output after subsequent operations accordingly as shown in the following equation where you determine the size of an image after padding and then applying a convolutions kernel of size K.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$M'=M+2 \\\\times padding$$\n",
    "$$M\\_{new}=M'-K+1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z5: tensor([[[[0., 0., 0.],\n",
      "          [0., 4., 0.],\n",
      "          [0., 0., 0.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "z5: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# padding adds x number of zero columns and rows, therefore new formula to calculate the ouput size becomes\n",
    "# I_ = I + 2*(padding)\n",
    "# O = (I_-K)/S +1\n",
    "conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=2)\n",
    "\n",
    "conv5.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv5.state_dict()['bias'][0]=0.0\n",
    "conv5.state_dict()\n",
    "z5=conv5(image1)\n",
    "print(\"z5:\",z5)\n",
    "print(\"z5:\",z4.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2513,  0.7812,  1.5445,  1.2977],\n",
       "          [-1.0773,  0.9248,  0.3427, -1.8953],\n",
       "          [ 0.2793,  0.8182, -0.3627, -0.0558],\n",
       "          [-1.6975,  0.3798, -0.4873, -0.2117]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image=torch.randn((1,1,4,4))\n",
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
    "conv.state_dict()['weight'][0][0]=torch.tensor([[0,0,0],[0,0,0],[0,0.0,0]])\n",
    "conv.state_dict()['bias'][0]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0.],\n",
       "          [0., 0.]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = conv(Image)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
